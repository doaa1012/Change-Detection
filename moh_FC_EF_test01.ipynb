{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BQ9g8Susd4nN",
        "4dyufVRFeSQv"
      ],
      "authorship_tag": "ABX9TyMhO2GQweM9gRNDajFH+9+h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cudaMBI/cudaMBI-Land-Cover-Change-Detection/blob/main/CDD_V1_2022-12-08/moh_FC_EF_test01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SARiiLrqQm-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef7eb1d6-e827-49e4-f6d2-2cb9c420ae62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/CDD_V1_2022-12-08\")\n",
        "\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMlToVfMXVRY",
        "outputId": "7fffcbe3-be79-4b1e-8694-ec7b66c37f4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CDD_V1_2022-12-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "4ZHpTXaxQ0tB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac32f57-14c2-4d64-cb9c-2c8c32c60117"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log  metadata.json  models  __pycache__  train.py  utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/cudaMBI/cudaMBI-Land-Cover-Change-Detection\n",
        "#!rm -rf \"/content/cudaMBI-Land-Cover-Change-Detection\" "
      ],
      "metadata": {
        "id": "JFV4GiWFRCzn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZCNjbxCUsdZ",
        "outputId": "b821466b-90ee-4d6a-e9db-02831b2155ae"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.8/dist-packages (2.5.1)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.8/dist-packages (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train file content"
      ],
      "metadata": {
        "id": "R015Pjm_X8yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import logging\n",
        "import json\n",
        "from tensorboardX import SummaryWriter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "from utils.helper import (get_loaders, load_model, get_criterion, initialize_metrics,\n",
        "                   set_metrics, get_mean_metrics)\n",
        "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
        "\n",
        "\"\"\"\n",
        "Initialize Parser and define arguments\n",
        "\n",
        "import parse\n",
        "from parse import get_parser_with_args\n",
        "\n",
        "parser, metadata = get_parser_with_args()\n",
        "opt = parser.parse_args()\n",
        "\"\"\"\n",
        "opt = {\"augmentation\": False,\n",
        "        \"num_gpus\": 1,\n",
        "        \"num_workers\": 0,\n",
        "        \"in_channel\": 6,\n",
        "        \"out_channel\":1,\n",
        "        'loss_function':'IoULoss',\n",
        "        \"epochs\": 2,\n",
        "        \"batch_size\": 16,\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"dataset_dir\": \"/content/drive/MyDrive/CDD_Data_2000imgs/\",\n",
        "        \"log_dir\": \"./log/\"\n",
        "      }\n",
        "\n",
        "\"\"\"\n",
        "Initialize experiments log\n",
        "\"\"\"\n",
        "writer = SummaryWriter(opt['log_dir'] + f'/FC_EF_{datetime.datetime.now().strftime(\"%Y-%m-%d\")}/')\n",
        "\n",
        "\"\"\"\n",
        "Set up environment: define paths, download data, and set device\n",
        "\"\"\"\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "dev = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('GPU AVAILABLE? ' + str(torch.cuda.is_available()))\n",
        "\n",
        "def seed_torch(seed=0):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_torch(seed=777)\n",
        "\n",
        "train_loader, val_loader = get_loaders(opt)\n",
        "\n",
        "\"\"\"\n",
        "Load Model then define other aspects of the model\n",
        "\"\"\"\n",
        "print('LOADING Model')\n",
        "model = load_model(opt, dev)\n",
        "\n",
        "criterion = get_criterion(opt)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=opt['learning_rate']) # Be careful when you adjust learning rate, you can refer to the linear scaling rule\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
        "\n",
        "\"\"\"\n",
        " Set starting values\n",
        "\"\"\"\n",
        "best_metrics = {'cd_f1scores': -1, 'cd_recalls': -1, 'cd_precisions': -1}\n",
        "logging.info('STARTING training')\n",
        "total_step = -1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQEXq_p9YAjH",
        "outputId": "fcfc0788-573f-4636-8bc1-0a3a887873c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU AVAILABLE? True\n",
            "LOADING Model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_img1, batch_img2, label = next(iter(train_loader))\n",
        "print(f\"batch image 1 size: {batch_img1.size()}\")\n",
        "print(f\"batch image 2 size: {batch_img2.size()}\")\n",
        "print(f\"label size: {label.size()}\")\n",
        "\n",
        "# we need to add extra dimension in label \n",
        "label = torch.unsqueeze(label, dim=1)\n",
        "print(f\"label size: {label.size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aXme35ThN9C",
        "outputId": "b518beed-044e-4ba7-fa8e-560f341b3a46"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch image 1 size: torch.Size([16, 3, 256, 256])\n",
            "batch image 2 size: torch.Size([16, 3, 256, 256])\n",
            "label size: torch.Size([16, 256, 256])\n",
            "label size: torch.Size([16, 1, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model functions"
      ],
      "metadata": {
        "id": "BQ9g8Susd4nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchinfo import summary\n",
        "\n",
        "# A placeholder identity operator that is argument-insensitive.\n",
        "Identity = nn.Identity\n",
        "\n",
        "\n",
        "def get_norm_layer():\n",
        "    # TODO: select appropriate norm layer\n",
        "    return nn.BatchNorm2d\n",
        "\n",
        "def get_act_layer():\n",
        "    # TODO: select appropriate activation layer\n",
        "    return nn.ReLU\n",
        "\n",
        "\n",
        "def make_norm(*args, **kwargs):\n",
        "    norm_layer = get_norm_layer()\n",
        "    return norm_layer(*args, **kwargs)\n",
        "\n",
        "def make_act(*args, **kwargs):\n",
        "    act_layer = get_act_layer()\n",
        "    return act_layer(*args, **kwargs)\n",
        "\n",
        "def make_dropout(use_dropout=False):\n",
        "        if use_dropout:\n",
        "            return nn.Dropout2d(p=0.2)\n",
        "        else:\n",
        "            return Identity()\n",
        "\n",
        "class BasicConv(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_ch, out_ch,\n",
        "            kernel_size, pad_mode='Zero',\n",
        "            bias='auto', norm=False, act=False, **kwargs\n",
        "        ):\n",
        "        super().__init__()\n",
        "        seq = []\n",
        "        if kernel_size >= 2:\n",
        "            seq.append( getattr(nn, pad_mode.capitalize()+'Pad2d')(kernel_size//2) )\n",
        "        seq.append(\n",
        "            nn.Conv2d(\n",
        "                in_ch, out_ch, kernel_size, stride=1, padding=0,\n",
        "                bias=(False if norm else True) if bias=='auto' else bias,\n",
        "                **kwargs\n",
        "            )    \n",
        "        )\n",
        "        if norm:\n",
        "            if norm is True:\n",
        "                norm = make_norm(out_ch)\n",
        "            seq.append(norm)\n",
        "        if act:\n",
        "            if act is True:\n",
        "                act = make_act()\n",
        "            seq.append(act)\n",
        "        self.seq = nn.Sequential(*seq)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        " \n",
        "       \n",
        "class Conv3x3(BasicConv):\n",
        "    def __init__(self, in_ch, out_ch, pad_mode='Zero', bias='auto', norm=False, act=False, **kwargs):\n",
        "        super().__init__(in_ch, out_ch, kernel_size=3, pad_mode=pad_mode, bias=bias, norm=norm, act=act, **kwargs)\n",
        "\n",
        "class MaxPool2x2(nn.MaxPool2d):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(kernel_size=2, stride=(2,2), padding=(0,0), **kwargs)\n",
        "\n",
        "class MaxUnPool2x2(nn.MaxUnpool2d):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(kernel_size=2, stride=(2,2), padding=(0,0), **kwargs)\n",
        "        \n",
        "\n",
        "class ConvTransposed3x3(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bias='auto', norm=False, act=False, **kwargs):\n",
        "        super().__init__()\n",
        "        seq = []\n",
        "        seq.append(\n",
        "                nn.ConvTranspose2d(in_ch, out_ch, kernel_size=3, stride=2, padding=1,\n",
        "                                   bias=(False if norm else True) if bias=='auto' else bias,\n",
        "                                   **kwargs\n",
        "                        )\n",
        "            )        \n",
        "        if norm:\n",
        "            if norm is True:\n",
        "                norm = make_norm(out_ch)\n",
        "            seq.append(norm)\n",
        "        if act:\n",
        "            if act is True:\n",
        "                act = make_act()\n",
        "            seq.append(act)\n",
        "        self.seq = nn.Sequential(*seq) \n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.seq(x)"
      ],
      "metadata": {
        "id": "ka9qV23Rd-es"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model"
      ],
      "metadata": {
        "id": "4dyufVRFeSQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, use_dropout=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.use_dropout = use_dropout\n",
        "        \n",
        "        self.conv11 = Conv3x3(in_ch, out_ch=16, norm=True, act=True)\n",
        "        self.do11 = self.make_dropout()\n",
        "        self.conv12 = Conv3x3(16, 16, norm=True, act=True)\n",
        "        self.do12 = self.make_dropout()\n",
        "        self.pool1 = MaxPool2x2()\n",
        "        \n",
        "        self.conv21 = Conv3x3(16, 32, norm=True, act=True)\n",
        "        self.do21 = self.make_dropout()\n",
        "        self.conv22 = Conv3x3(32, 32, norm=True, act=True)\n",
        "        self.do22 = self.make_dropout()\n",
        "        self.pool2 = MaxPool2x2()\n",
        "\n",
        "        self.conv31 = Conv3x3(32, 64, norm=True, act=True)\n",
        "        self.do31 = self.make_dropout()\n",
        "        self.conv32 = Conv3x3(64, 64, norm=True, act=True)\n",
        "        self.do32 = self.make_dropout()\n",
        "        self.conv33 = Conv3x3(64, 64, norm=True, act=True)\n",
        "        self.do33 = self.make_dropout()\n",
        "        self.pool3 = MaxPool2x2()\n",
        "\n",
        "        self.conv41 = Conv3x3(64, 128, norm=True, act=True)\n",
        "        self.do41 = self.make_dropout()\n",
        "        self.conv42 = Conv3x3(128, 128, norm=True, act=True)\n",
        "        self.do42 = self.make_dropout()\n",
        "        self.conv43 = Conv3x3(128, 128, norm=True, act=True)\n",
        "        self.do43 = self.make_dropout()\n",
        "        self.pool4 = MaxPool2x2()\n",
        "\n",
        "        self.upconv4 = ConvTransposed3x3(128, 128, output_padding=1)\n",
        "\n",
        "        self.conv43d = Conv3x3(256, 128, norm=True, act=True)\n",
        "        self.do43d = self.make_dropout()\n",
        "        self.conv42d = Conv3x3(128, 128, norm=True, act=True)\n",
        "        self.do42d = self.make_dropout()\n",
        "        self.conv41d = Conv3x3(128, 64, norm=True, act=True)\n",
        "        self.do41d = self.make_dropout()\n",
        "\n",
        "        self.upconv3 = ConvTransposed3x3(64, 64, output_padding=1)\n",
        "\n",
        "        self.conv33d = Conv3x3(128, 64, norm=True, act=True)\n",
        "        self.do33d = self.make_dropout()\n",
        "        self.conv32d = Conv3x3(64, 64, norm=True, act=True)\n",
        "        self.do32d = self.make_dropout()\n",
        "        self.conv31d = Conv3x3(64, 32, norm=True, act=True)\n",
        "        self.do31d = self.make_dropout()\n",
        "\n",
        "        self.upconv2 = ConvTransposed3x3(in_ch=32, out_ch=32, output_padding=1)\n",
        "\n",
        "        self.conv22d = Conv3x3(64, 32, norm=True, act=True)\n",
        "        self.do22d = self.make_dropout()\n",
        "        self.conv21d = Conv3x3(32, 16, norm=True, act=True)\n",
        "        self.do21d = self.make_dropout()\n",
        "\n",
        "        self.upconv1 = ConvTransposed3x3(in_ch=16, out_ch=16, output_padding=1)\n",
        "\n",
        "        self.conv12d = Conv3x3(32, 16, norm=True, act=True)\n",
        "        self.do12d = self.make_dropout()\n",
        "        self.conv11d = Conv3x3(16, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = torch.cat([t1, t2], dim=1)\n",
        "        \n",
        "        # Stage 1\n",
        "        x11 = self.do11(self.conv11(x))\n",
        "        x12 = self.do12(self.conv12(x11))\n",
        "        x1p = self.pool1(x12)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(self.conv21(x1p))\n",
        "        x22 = self.do22(self.conv22(x21))\n",
        "        x2p = self.pool2(x22)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(self.conv31(x2p))\n",
        "        x32 = self.do32(self.conv32(x31))\n",
        "        x33 = self.do33(self.conv33(x32))\n",
        "        x3p = self.pool3(x33)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(self.conv41(x3p))\n",
        "        x42 = self.do42(self.conv42(x41))\n",
        "        x43 = self.do43(self.conv43(x42))\n",
        "        x4p = self.pool4(x43)\n",
        "\n",
        "        # Stage 4d\n",
        "        x4d = self.upconv4(x4p)\n",
        "        pad4 = (0, x43.shape[3]-x4d.shape[3], 0, x43.shape[2]-x4d.shape[2])\n",
        "        x4d = torch.cat([F.pad(x4d, pad=pad4, mode='replicate'), x43], 1)\n",
        "        x43d = self.do43d(self.conv43d(x4d))\n",
        "        x42d = self.do42d(self.conv42d(x43d))\n",
        "        x41d = self.do41d(self.conv41d(x42d))\n",
        "\n",
        "        # Stage 3d\n",
        "        x3d = self.upconv3(x41d)\n",
        "        pad3 = (0, x33.shape[3]-x3d.shape[3], 0, x33.shape[2]-x3d.shape[2])\n",
        "        x3d = torch.cat([F.pad(x3d, pad=pad3, mode='replicate'), x33], 1)\n",
        "        x33d = self.do33d(self.conv33d(x3d))\n",
        "        x32d = self.do32d(self.conv32d(x33d))\n",
        "        x31d = self.do31d(self.conv31d(x32d))\n",
        "\n",
        "        # Stage 2d\n",
        "        x2d = self.upconv2(x31d)\n",
        "        pad2 = (0, x22.shape[3]-x2d.shape[3], 0, x22.shape[2]-x2d.shape[2])\n",
        "        x2d = torch.cat([F.pad(x2d, pad=pad2, mode='replicate'), x22], 1)\n",
        "        x22d = self.do22d(self.conv22d(x2d))\n",
        "        x21d = self.do21d(self.conv21d(x22d))\n",
        "\n",
        "        # Stage 1d\n",
        "        x1d = self.upconv1(x21d)\n",
        "        pad1 = (0, x12.shape[3]-x1d.shape[3], 0, x12.shape[2]-x1d.shape[2])\n",
        "        x1d = torch.cat([F.pad(x1d, pad=pad1, mode='replicate'), x12], 1)\n",
        "        x12d = self.do12d(self.conv12d(x1d))\n",
        "        x11d = self.conv11d(x12d)\n",
        "\n",
        "        return x11d\n",
        "        \n",
        "        \n",
        "    def make_dropout(self):\n",
        "        if self.use_dropout:\n",
        "            return nn.Dropout2d(p=0.2)\n",
        "        else:\n",
        "            return Identity()\n",
        "            \n",
        "unet_model = UNet(in_ch=6, out_ch=2, use_dropout=False)\n",
        "print(summary(unet_model, input_size=((64, 6, 256, 256))))            "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deyulf4KdR5G",
        "outputId": "90dbcbf5-2292-4fb2-cc3f-176453a609af"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "UNet                                     [64, 2, 256, 256]         --\n",
            "├─Conv3x3: 1-1                           [64, 16, 256, 256]        --\n",
            "│    └─Sequential: 2-1                   [64, 16, 256, 256]        --\n",
            "│    │    └─ZeroPad2d: 3-1               [64, 6, 258, 258]         --\n",
            "│    │    └─Conv2d: 3-2                  [64, 16, 256, 256]        864\n",
            "│    │    └─BatchNorm2d: 3-3             [64, 16, 256, 256]        32\n",
            "│    │    └─ReLU: 3-4                    [64, 16, 256, 256]        --\n",
            "├─Identity: 1-2                          [64, 16, 256, 256]        --\n",
            "├─Conv3x3: 1-3                           [64, 16, 256, 256]        --\n",
            "│    └─Sequential: 2-2                   [64, 16, 256, 256]        --\n",
            "│    │    └─ZeroPad2d: 3-5               [64, 16, 258, 258]        --\n",
            "│    │    └─Conv2d: 3-6                  [64, 16, 256, 256]        2,304\n",
            "│    │    └─BatchNorm2d: 3-7             [64, 16, 256, 256]        32\n",
            "│    │    └─ReLU: 3-8                    [64, 16, 256, 256]        --\n",
            "├─Identity: 1-4                          [64, 16, 256, 256]        --\n",
            "├─MaxPool2x2: 1-5                        [64, 16, 128, 128]        --\n",
            "├─Conv3x3: 1-6                           [64, 32, 128, 128]        --\n",
            "│    └─Sequential: 2-3                   [64, 32, 128, 128]        --\n",
            "│    │    └─ZeroPad2d: 3-9               [64, 16, 130, 130]        --\n",
            "│    │    └─Conv2d: 3-10                 [64, 32, 128, 128]        4,608\n",
            "│    │    └─BatchNorm2d: 3-11            [64, 32, 128, 128]        64\n",
            "│    │    └─ReLU: 3-12                   [64, 32, 128, 128]        --\n",
            "├─Identity: 1-7                          [64, 32, 128, 128]        --\n",
            "├─Conv3x3: 1-8                           [64, 32, 128, 128]        --\n",
            "│    └─Sequential: 2-4                   [64, 32, 128, 128]        --\n",
            "│    │    └─ZeroPad2d: 3-13              [64, 32, 130, 130]        --\n",
            "│    │    └─Conv2d: 3-14                 [64, 32, 128, 128]        9,216\n",
            "│    │    └─BatchNorm2d: 3-15            [64, 32, 128, 128]        64\n",
            "│    │    └─ReLU: 3-16                   [64, 32, 128, 128]        --\n",
            "├─Identity: 1-9                          [64, 32, 128, 128]        --\n",
            "├─MaxPool2x2: 1-10                       [64, 32, 64, 64]          --\n",
            "├─Conv3x3: 1-11                          [64, 64, 64, 64]          --\n",
            "│    └─Sequential: 2-5                   [64, 64, 64, 64]          --\n",
            "│    │    └─ZeroPad2d: 3-17              [64, 32, 66, 66]          --\n",
            "│    │    └─Conv2d: 3-18                 [64, 64, 64, 64]          18,432\n",
            "│    │    └─BatchNorm2d: 3-19            [64, 64, 64, 64]          128\n",
            "│    │    └─ReLU: 3-20                   [64, 64, 64, 64]          --\n",
            "├─Identity: 1-12                         [64, 64, 64, 64]          --\n",
            "├─Conv3x3: 1-13                          [64, 64, 64, 64]          --\n",
            "│    └─Sequential: 2-6                   [64, 64, 64, 64]          --\n",
            "│    │    └─ZeroPad2d: 3-21              [64, 64, 66, 66]          --\n",
            "│    │    └─Conv2d: 3-22                 [64, 64, 64, 64]          36,864\n",
            "│    │    └─BatchNorm2d: 3-23            [64, 64, 64, 64]          128\n",
            "│    │    └─ReLU: 3-24                   [64, 64, 64, 64]          --\n",
            "├─Identity: 1-14                         [64, 64, 64, 64]          --\n",
            "├─Conv3x3: 1-15                          [64, 64, 64, 64]          --\n",
            "│    └─Sequential: 2-7                   [64, 64, 64, 64]          --\n",
            "│    │    └─ZeroPad2d: 3-25              [64, 64, 66, 66]          --\n",
            "│    │    └─Conv2d: 3-26                 [64, 64, 64, 64]          36,864\n",
            "│    │    └─BatchNorm2d: 3-27            [64, 64, 64, 64]          128\n",
            "│    │    └─ReLU: 3-28                   [64, 64, 64, 64]          --\n",
            "├─Identity: 1-16                         [64, 64, 64, 64]          --\n",
            "├─MaxPool2x2: 1-17                       [64, 64, 32, 32]          --\n",
            "├─Conv3x3: 1-18                          [64, 128, 32, 32]         --\n",
            "│    └─Sequential: 2-8                   [64, 128, 32, 32]         --\n",
            "│    │    └─ZeroPad2d: 3-29              [64, 64, 34, 34]          --\n",
            "│    │    └─Conv2d: 3-30                 [64, 128, 32, 32]         73,728\n",
            "│    │    └─BatchNorm2d: 3-31            [64, 128, 32, 32]         256\n",
            "│    │    └─ReLU: 3-32                   [64, 128, 32, 32]         --\n",
            "├─Identity: 1-19                         [64, 128, 32, 32]         --\n",
            "├─Conv3x3: 1-20                          [64, 128, 32, 32]         --\n",
            "│    └─Sequential: 2-9                   [64, 128, 32, 32]         --\n",
            "│    │    └─ZeroPad2d: 3-33              [64, 128, 34, 34]         --\n",
            "│    │    └─Conv2d: 3-34                 [64, 128, 32, 32]         147,456\n",
            "│    │    └─BatchNorm2d: 3-35            [64, 128, 32, 32]         256\n",
            "│    │    └─ReLU: 3-36                   [64, 128, 32, 32]         --\n",
            "├─Identity: 1-21                         [64, 128, 32, 32]         --\n",
            "├─Conv3x3: 1-22                          [64, 128, 32, 32]         --\n",
            "│    └─Sequential: 2-10                  [64, 128, 32, 32]         --\n",
            "│    │    └─ZeroPad2d: 3-37              [64, 128, 34, 34]         --\n",
            "│    │    └─Conv2d: 3-38                 [64, 128, 32, 32]         147,456\n",
            "│    │    └─BatchNorm2d: 3-39            [64, 128, 32, 32]         256\n",
            "│    │    └─ReLU: 3-40                   [64, 128, 32, 32]         --\n",
            "├─Identity: 1-23                         [64, 128, 32, 32]         --\n",
            "├─MaxPool2x2: 1-24                       [64, 128, 16, 16]         --\n",
            "├─ConvTransposed3x3: 1-25                [64, 128, 32, 32]         --\n",
            "│    └─Sequential: 2-11                  [64, 128, 32, 32]         --\n",
            "│    │    └─ConvTranspose2d: 3-41        [64, 128, 32, 32]         147,584\n",
            "├─Conv3x3: 1-26                          [64, 128, 32, 32]         --\n",
            "│    └─Sequential: 2-12                  [64, 128, 32, 32]         --\n",
            "│    │    └─ZeroPad2d: 3-42              [64, 256, 34, 34]         --\n",
            "│    │    └─Conv2d: 3-43                 [64, 128, 32, 32]         294,912\n",
            "│    │    └─BatchNorm2d: 3-44            [64, 128, 32, 32]         256\n",
            "│    │    └─ReLU: 3-45                   [64, 128, 32, 32]         --\n",
            "├─Identity: 1-27                         [64, 128, 32, 32]         --\n",
            "├─Conv3x3: 1-28                          [64, 128, 32, 32]         --\n",
            "│    └─Sequential: 2-13                  [64, 128, 32, 32]         --\n",
            "│    │    └─ZeroPad2d: 3-46              [64, 128, 34, 34]         --\n",
            "│    │    └─Conv2d: 3-47                 [64, 128, 32, 32]         147,456\n",
            "│    │    └─BatchNorm2d: 3-48            [64, 128, 32, 32]         256\n",
            "│    │    └─ReLU: 3-49                   [64, 128, 32, 32]         --\n",
            "├─Identity: 1-29                         [64, 128, 32, 32]         --\n",
            "├─Conv3x3: 1-30                          [64, 64, 32, 32]          --\n",
            "│    └─Sequential: 2-14                  [64, 64, 32, 32]          --\n",
            "│    │    └─ZeroPad2d: 3-50              [64, 128, 34, 34]         --\n",
            "│    │    └─Conv2d: 3-51                 [64, 64, 32, 32]          73,728\n",
            "│    │    └─BatchNorm2d: 3-52            [64, 64, 32, 32]          128\n",
            "│    │    └─ReLU: 3-53                   [64, 64, 32, 32]          --\n",
            "├─Identity: 1-31                         [64, 64, 32, 32]          --\n",
            "├─ConvTransposed3x3: 1-32                [64, 64, 64, 64]          --\n",
            "│    └─Sequential: 2-15                  [64, 64, 64, 64]          --\n",
            "│    │    └─ConvTranspose2d: 3-54        [64, 64, 64, 64]          36,928\n",
            "├─Conv3x3: 1-33                          [64, 64, 64, 64]          --\n",
            "│    └─Sequential: 2-16                  [64, 64, 64, 64]          --\n",
            "│    │    └─ZeroPad2d: 3-55              [64, 128, 66, 66]         --\n",
            "│    │    └─Conv2d: 3-56                 [64, 64, 64, 64]          73,728\n",
            "│    │    └─BatchNorm2d: 3-57            [64, 64, 64, 64]          128\n",
            "│    │    └─ReLU: 3-58                   [64, 64, 64, 64]          --\n",
            "├─Identity: 1-34                         [64, 64, 64, 64]          --\n",
            "├─Conv3x3: 1-35                          [64, 64, 64, 64]          --\n",
            "│    └─Sequential: 2-17                  [64, 64, 64, 64]          --\n",
            "│    │    └─ZeroPad2d: 3-59              [64, 64, 66, 66]          --\n",
            "│    │    └─Conv2d: 3-60                 [64, 64, 64, 64]          36,864\n",
            "│    │    └─BatchNorm2d: 3-61            [64, 64, 64, 64]          128\n",
            "│    │    └─ReLU: 3-62                   [64, 64, 64, 64]          --\n",
            "├─Identity: 1-36                         [64, 64, 64, 64]          --\n",
            "├─Conv3x3: 1-37                          [64, 32, 64, 64]          --\n",
            "│    └─Sequential: 2-18                  [64, 32, 64, 64]          --\n",
            "│    │    └─ZeroPad2d: 3-63              [64, 64, 66, 66]          --\n",
            "│    │    └─Conv2d: 3-64                 [64, 32, 64, 64]          18,432\n",
            "│    │    └─BatchNorm2d: 3-65            [64, 32, 64, 64]          64\n",
            "│    │    └─ReLU: 3-66                   [64, 32, 64, 64]          --\n",
            "├─Identity: 1-38                         [64, 32, 64, 64]          --\n",
            "├─ConvTransposed3x3: 1-39                [64, 32, 128, 128]        --\n",
            "│    └─Sequential: 2-19                  [64, 32, 128, 128]        --\n",
            "│    │    └─ConvTranspose2d: 3-67        [64, 32, 128, 128]        9,248\n",
            "├─Conv3x3: 1-40                          [64, 32, 128, 128]        --\n",
            "│    └─Sequential: 2-20                  [64, 32, 128, 128]        --\n",
            "│    │    └─ZeroPad2d: 3-68              [64, 64, 130, 130]        --\n",
            "│    │    └─Conv2d: 3-69                 [64, 32, 128, 128]        18,432\n",
            "│    │    └─BatchNorm2d: 3-70            [64, 32, 128, 128]        64\n",
            "│    │    └─ReLU: 3-71                   [64, 32, 128, 128]        --\n",
            "├─Identity: 1-41                         [64, 32, 128, 128]        --\n",
            "├─Conv3x3: 1-42                          [64, 16, 128, 128]        --\n",
            "│    └─Sequential: 2-21                  [64, 16, 128, 128]        --\n",
            "│    │    └─ZeroPad2d: 3-72              [64, 32, 130, 130]        --\n",
            "│    │    └─Conv2d: 3-73                 [64, 16, 128, 128]        4,608\n",
            "│    │    └─BatchNorm2d: 3-74            [64, 16, 128, 128]        32\n",
            "│    │    └─ReLU: 3-75                   [64, 16, 128, 128]        --\n",
            "├─Identity: 1-43                         [64, 16, 128, 128]        --\n",
            "├─ConvTransposed3x3: 1-44                [64, 16, 256, 256]        --\n",
            "│    └─Sequential: 2-22                  [64, 16, 256, 256]        --\n",
            "│    │    └─ConvTranspose2d: 3-76        [64, 16, 256, 256]        2,320\n",
            "├─Conv3x3: 1-45                          [64, 16, 256, 256]        --\n",
            "│    └─Sequential: 2-23                  [64, 16, 256, 256]        --\n",
            "│    │    └─ZeroPad2d: 3-77              [64, 32, 258, 258]        --\n",
            "│    │    └─Conv2d: 3-78                 [64, 16, 256, 256]        4,608\n",
            "│    │    └─BatchNorm2d: 3-79            [64, 16, 256, 256]        32\n",
            "│    │    └─ReLU: 3-80                   [64, 16, 256, 256]        --\n",
            "├─Identity: 1-46                         [64, 16, 256, 256]        --\n",
            "├─Conv3x3: 1-47                          [64, 2, 256, 256]         --\n",
            "│    └─Sequential: 2-24                  [64, 2, 256, 256]         --\n",
            "│    │    └─ZeroPad2d: 3-81              [64, 16, 258, 258]        --\n",
            "│    │    └─Conv2d: 3-82                 [64, 2, 256, 256]         290\n",
            "==========================================================================================\n",
            "Total params: 1,349,362\n",
            "Trainable params: 1,349,362\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 227.23\n",
            "==========================================================================================\n",
            "Input size (MB): 100.66\n",
            "Forward/backward pass size (MB): 8388.61\n",
            "Params size (MB): 5.40\n",
            "Estimated Total Size (MB): 8494.67\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.cat([batch_img1, batch_img1], dim=1)\n",
        "x.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFtx-stKgqL1",
        "outputId": "f407db76-3e49-4fe5-ab99-28bb3beef1af"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 6, 256, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training loop"
      ],
      "metadata": {
        "id": "YMQWBya5d1yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_img1, batch_img2, labels = next(iter(train_loader))\n",
        "print(f\"batch image 1 size: {batch_img1.size()}\")\n",
        "print(f\"batch image 2 size: {batch_img2.size()}\")\n",
        "print(f\"label size: {labels.size()}\") # we need to add extra dimension in label \n",
        "\n",
        "labels = torch.unsqueeze(labels, dim=1)\n",
        "labels = labels.long().to(dev)\n",
        "print(f\"label size: {labels.size()}\")\n",
        "\n",
        "inputs = torch.cat([batch_img1, batch_img1], dim=1).to(dev)\n",
        "print(f\"concate batch 1 and batch 2 images: {inputs.size()}\")\n",
        "\n",
        "model = model.to(dev)\n",
        "\n",
        "cd_preds = model(inputs)\n",
        "print(f\"output size: {cd_preds.size()}\")\n",
        "\n",
        "print(cd_preds[-1].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mewKar1CmgVq",
        "outputId": "4b856ce5-f2be-4623-8647-11ec84e59c2e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch image 1 size: torch.Size([16, 3, 256, 256])\n",
            "batch image 2 size: torch.Size([16, 3, 256, 256])\n",
            "label size: torch.Size([16, 256, 256])\n",
            "label size: torch.Size([16, 1, 256, 256])\n",
            "concate batch 1 and batch 2 images: torch.Size([16, 6, 256, 256])\n",
            "output size: torch.Size([16, 1, 256, 256])\n",
            "torch.Size([1, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t65th5WnrVTQ",
        "outputId": "264888c5-4d76-4035-b703-0f2dc229487c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 1, 1, 1],\n",
            "         [0, 0, 0,  ..., 1, 1, 1],\n",
            "         [0, 0, 0,  ..., 1, 1, 1]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd_preds = model(inputs)\n",
        "print(cd_preds[-1])\n",
        "\n",
        "_, cd_preds = torch.max(cd_preds, 1)\n",
        "print(cd_preds)\n",
        "print(cd_preds.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6blV9mZpZvA",
        "outputId": "0045e938-ffbb-4a98-c3ce-5fa1e4006b8d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.1570, 0.1698, 0.1766,  ..., 0.1441, 0.0974, 0.0494],\n",
            "         [0.3115, 0.3727, 0.4215,  ..., 0.3042, 0.2431, 0.0597],\n",
            "         [0.3702, 0.4333, 0.5130,  ..., 0.3504, 0.2834, 0.0868],\n",
            "         ...,\n",
            "         [0.2603, 0.3804, 0.4135,  ..., 0.3281, 0.2463, 0.0799],\n",
            "         [0.2627, 0.4113, 0.4421,  ..., 0.3560, 0.2892, 0.1354],\n",
            "         [0.1902, 0.3297, 0.3475,  ..., 0.2474, 0.2135, 0.1074]]],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "torch.Size([16, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(opt['epochs']):  # loop over the dataset multiple times\n",
        "    \n",
        "    train_metrics = initialize_metrics()\n",
        "    val_metrics = initialize_metrics()\n",
        "\n",
        "    \"\"\"\n",
        "    Begin Training\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    print('SET model mode to train!')\n",
        "    batch_iter = 0\n",
        "    #tbar = tqdm(train_loader)\n",
        "\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [batch_img1, batch_img2, labels]\n",
        "        batch_img1, batch_img2, labels = data\n",
        "        \n",
        "        # Set variables for training\n",
        "        batch_img1 = batch_img1.float()\n",
        "        batch_img2 = batch_img2.float()\n",
        "        \n",
        "        inputs = torch.cat([batch_img1, batch_img2], dim=1).to(dev)\n",
        "\n",
        "        #labels = torch.unsqueeze(labels, dim=1)\n",
        "        labels = labels.long().to(dev)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        # Get model predictions, calculate loss, backprop\n",
        "        cd_preds = model(inputs)\n",
        "\n",
        "        cd_loss = criterion(cd_preds, labels)\n",
        "        loss = cd_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        cd_preds = cd_preds[-1]\n",
        "        _, cd_preds = torch.max(cd_preds, 1)\n",
        "        \n",
        "\n",
        "        # clear batch variables from memory\n",
        "        del batch_img1, batch_img2, inputs, labels\n",
        "        \n",
        "        \n",
        "        # print statistics\n",
        "        print(f\"[{epoch + 1}, {i + 1:5d}] loss: {cd_loss:.5f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "    logging.info(\"EPOCH {} TRAIN METRICS\".format(epoch) + str(cd_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffoc-KKKodvp",
        "outputId": "b8845e51-ba8b-4e85-d7e3-f489aa193831"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SET model mode to train!\n",
            "[1,     1] loss: 1.01762\n",
            "[1,     2] loss: 0.86002\n",
            "[1,     3] loss: 0.78050\n",
            "[1,     4] loss: 0.85326\n",
            "[1,     5] loss: -0.24153\n",
            "[1,     6] loss: 0.40890\n",
            "[1,     7] loss: -0.35814\n",
            "[1,     8] loss: -1.73874\n",
            "[1,     9] loss: 0.00880\n",
            "[1,    10] loss: 1.95581\n",
            "[1,    11] loss: -0.76622\n",
            "[1,    12] loss: 1.29365\n",
            "[1,    13] loss: 0.98158\n",
            "[1,    14] loss: 1.27434\n",
            "[1,    15] loss: 1.00376\n",
            "[1,    16] loss: 1.14439\n",
            "[1,    17] loss: 0.92594\n",
            "[1,    18] loss: 1.04532\n",
            "[1,    19] loss: 1.13380\n",
            "[1,    20] loss: 1.00727\n",
            "[1,    21] loss: 0.96440\n",
            "[1,    22] loss: 0.94631\n",
            "[1,    23] loss: 0.90711\n",
            "[1,    24] loss: 0.87282\n",
            "[1,    25] loss: 0.94549\n",
            "[1,    26] loss: 0.96340\n",
            "[1,    27] loss: 0.92988\n",
            "[1,    28] loss: 0.92060\n",
            "[1,    29] loss: 0.91616\n",
            "[1,    30] loss: 0.89790\n",
            "[1,    31] loss: 0.88083\n",
            "[1,    32] loss: 0.88495\n",
            "[1,    33] loss: 0.85797\n",
            "[1,    34] loss: 0.90238\n",
            "[1,    35] loss: 0.92786\n",
            "[1,    36] loss: 0.81383\n",
            "[1,    37] loss: 0.57348\n",
            "[1,    38] loss: 0.59839\n",
            "[1,    39] loss: 0.79959\n",
            "[1,    40] loss: 0.74264\n",
            "[1,    41] loss: 0.57473\n",
            "[1,    42] loss: 0.30200\n",
            "[1,    43] loss: 0.95984\n",
            "[1,    44] loss: 0.92652\n",
            "[1,    45] loss: 0.22821\n",
            "[1,    46] loss: 0.93538\n",
            "[1,    47] loss: 0.96989\n",
            "[1,    48] loss: -0.20054\n",
            "[1,    49] loss: 0.76743\n",
            "[1,    50] loss: 0.38277\n",
            "[1,    51] loss: -59.32655\n",
            "[1,    52] loss: 0.51920\n",
            "[1,    53] loss: 0.95884\n",
            "[1,    54] loss: 5.69863\n",
            "[1,    55] loss: -0.07640\n",
            "[1,    56] loss: 5.08029\n",
            "[1,    57] loss: 0.28768\n",
            "[1,    58] loss: 0.85144\n",
            "[1,    59] loss: 1.45284\n",
            "[1,    60] loss: 0.85755\n",
            "[1,    61] loss: 0.96959\n",
            "[1,    62] loss: 0.90636\n",
            "[1,    63] loss: 2.32200\n",
            "[1,    64] loss: -3.23747\n",
            "[1,    65] loss: 1.06623\n",
            "[1,    66] loss: 1.10791\n",
            "[1,    67] loss: 1.04282\n",
            "[1,    68] loss: 0.99452\n",
            "[1,    69] loss: 0.98101\n",
            "[1,    70] loss: 0.96340\n",
            "[1,    71] loss: 0.99919\n",
            "[1,    72] loss: 0.95309\n",
            "[1,    73] loss: 0.94753\n",
            "[1,    74] loss: 0.91399\n",
            "[1,    75] loss: 0.96989\n",
            "[1,    76] loss: 0.99321\n",
            "[1,    77] loss: 0.90408\n",
            "[1,    78] loss: 0.92682\n",
            "[1,    79] loss: 0.97112\n",
            "[1,    80] loss: 0.92727\n",
            "[1,    81] loss: 0.96019\n",
            "[1,    82] loss: 0.91372\n",
            "[1,    83] loss: 0.94920\n",
            "[1,    84] loss: 0.96595\n",
            "[1,    85] loss: 0.92533\n",
            "[1,    86] loss: 0.94294\n",
            "[1,    87] loss: 0.93418\n",
            "[1,    88] loss: 0.87853\n",
            "[1,    89] loss: 0.91280\n",
            "[1,    90] loss: 0.93465\n",
            "[1,    91] loss: 0.96887\n",
            "[1,    92] loss: 0.89954\n",
            "[1,    93] loss: 0.94177\n",
            "[1,    94] loss: 0.95236\n",
            "[1,    95] loss: 0.96050\n",
            "[1,    96] loss: 0.76921\n",
            "[1,    97] loss: 0.92059\n",
            "[1,    98] loss: 0.91802\n",
            "[1,    99] loss: 0.93344\n",
            "[1,   100] loss: 0.92765\n",
            "[1,   101] loss: 0.91731\n",
            "[1,   102] loss: 0.97351\n",
            "[1,   103] loss: 0.93368\n",
            "[1,   104] loss: 0.96438\n",
            "[1,   105] loss: 0.92339\n",
            "[1,   106] loss: 0.93783\n",
            "[1,   107] loss: 0.94400\n",
            "[1,   108] loss: 0.96108\n",
            "[1,   109] loss: 0.96011\n",
            "[1,   110] loss: 0.91424\n",
            "[1,   111] loss: 0.92220\n",
            "[1,   112] loss: 0.85421\n",
            "[1,   113] loss: 0.97917\n",
            "[1,   114] loss: 0.91128\n",
            "[1,   115] loss: 0.96875\n",
            "[1,   116] loss: 0.90003\n",
            "[1,   117] loss: 0.94840\n",
            "[1,   118] loss: 0.91422\n",
            "[1,   119] loss: 0.96526\n",
            "[1,   120] loss: 0.95006\n",
            "[1,   121] loss: 0.95925\n",
            "[1,   122] loss: 0.94097\n",
            "[1,   123] loss: 0.97092\n",
            "[1,   124] loss: 0.94355\n",
            "[1,   125] loss: 0.95873\n",
            "SET model mode to train!\n",
            "[2,     1] loss: 0.89009\n",
            "[2,     2] loss: 0.94094\n",
            "[2,     3] loss: 0.92572\n",
            "[2,     4] loss: 0.91175\n",
            "[2,     5] loss: 0.94470\n",
            "[2,     6] loss: 0.93283\n",
            "[2,     7] loss: 0.88165\n",
            "[2,     8] loss: 0.92992\n",
            "[2,     9] loss: 0.92725\n",
            "[2,    10] loss: 0.93428\n",
            "[2,    11] loss: 0.93422\n",
            "[2,    12] loss: 0.94390\n",
            "[2,    13] loss: 0.95156\n",
            "[2,    14] loss: 0.91483\n",
            "[2,    15] loss: 0.95020\n",
            "[2,    16] loss: 0.92271\n",
            "[2,    17] loss: 0.92567\n",
            "[2,    18] loss: 0.85012\n",
            "[2,    19] loss: 0.86668\n",
            "[2,    20] loss: 0.92403\n",
            "[2,    21] loss: 0.92694\n",
            "[2,    22] loss: 0.95990\n",
            "[2,    23] loss: 0.93917\n",
            "[2,    24] loss: 0.91186\n",
            "[2,    25] loss: 0.94525\n",
            "[2,    26] loss: 0.92437\n",
            "[2,    27] loss: 0.89428\n",
            "[2,    28] loss: 0.92664\n",
            "[2,    29] loss: 0.89987\n",
            "[2,    30] loss: 0.95165\n",
            "[2,    31] loss: 0.92539\n",
            "[2,    32] loss: 0.91586\n",
            "[2,    33] loss: 0.87311\n",
            "[2,    34] loss: 0.91653\n",
            "[2,    35] loss: 0.93439\n",
            "[2,    36] loss: 0.93023\n",
            "[2,    37] loss: 0.96059\n",
            "[2,    38] loss: 0.91109\n",
            "[2,    39] loss: 0.96716\n",
            "[2,    40] loss: 0.96693\n",
            "[2,    41] loss: 0.91215\n",
            "[2,    42] loss: 0.96035\n",
            "[2,    43] loss: 0.92924\n",
            "[2,    44] loss: 0.94985\n",
            "[2,    45] loss: 0.94839\n",
            "[2,    46] loss: 0.93956\n",
            "[2,    47] loss: 0.93351\n",
            "[2,    48] loss: 0.92747\n",
            "[2,    49] loss: 0.95560\n",
            "[2,    50] loss: 0.94296\n",
            "[2,    51] loss: 0.89761\n",
            "[2,    52] loss: 0.95127\n",
            "[2,    53] loss: 0.94080\n",
            "[2,    54] loss: 0.90493\n",
            "[2,    55] loss: 0.95620\n",
            "[2,    56] loss: 0.91164\n",
            "[2,    57] loss: 0.94328\n",
            "[2,    58] loss: 0.85808\n",
            "[2,    59] loss: 0.94923\n",
            "[2,    60] loss: 0.92980\n",
            "[2,    61] loss: 0.93947\n",
            "[2,    62] loss: 0.89140\n",
            "[2,    63] loss: 0.93285\n",
            "[2,    64] loss: 0.88974\n",
            "[2,    65] loss: 0.91928\n",
            "[2,    66] loss: 0.84469\n",
            "[2,    67] loss: 0.88456\n",
            "[2,    68] loss: 0.93940\n",
            "[2,    69] loss: 0.94802\n",
            "[2,    70] loss: 0.91836\n",
            "[2,    71] loss: 0.91412\n",
            "[2,    72] loss: 0.94697\n",
            "[2,    73] loss: 0.90769\n",
            "[2,    74] loss: 0.93901\n",
            "[2,    75] loss: 0.91976\n",
            "[2,    76] loss: 0.93043\n",
            "[2,    77] loss: 0.94146\n",
            "[2,    78] loss: 0.90614\n",
            "[2,    79] loss: 0.95529\n",
            "[2,    80] loss: 0.95290\n",
            "[2,    81] loss: 0.86488\n",
            "[2,    82] loss: 0.91976\n",
            "[2,    83] loss: 0.87879\n",
            "[2,    84] loss: 0.92324\n",
            "[2,    85] loss: 0.93033\n",
            "[2,    86] loss: 0.93852\n",
            "[2,    87] loss: 0.87093\n",
            "[2,    88] loss: 0.93986\n",
            "[2,    89] loss: 0.91387\n",
            "[2,    90] loss: 0.96059\n",
            "[2,    91] loss: 0.87947\n",
            "[2,    92] loss: 0.97456\n",
            "[2,    93] loss: 0.94397\n",
            "[2,    94] loss: 0.90388\n",
            "[2,    95] loss: 0.93585\n",
            "[2,    96] loss: 0.96729\n",
            "[2,    97] loss: 0.90653\n",
            "[2,    98] loss: 0.93079\n",
            "[2,    99] loss: 0.89894\n",
            "[2,   100] loss: 0.89877\n",
            "[2,   101] loss: 0.86754\n",
            "[2,   102] loss: 0.95057\n",
            "[2,   103] loss: 0.91714\n",
            "[2,   104] loss: 0.96479\n",
            "[2,   105] loss: 0.95047\n",
            "[2,   106] loss: 0.95962\n",
            "[2,   107] loss: 0.89951\n",
            "[2,   108] loss: 0.92928\n",
            "[2,   109] loss: 0.93957\n",
            "[2,   110] loss: 0.94803\n",
            "[2,   111] loss: 0.95226\n",
            "[2,   112] loss: 0.91415\n",
            "[2,   113] loss: 0.95167\n",
            "[2,   114] loss: 0.87670\n",
            "[2,   115] loss: 0.93356\n",
            "[2,   116] loss: 0.91697\n",
            "[2,   117] loss: 0.93836\n",
            "[2,   118] loss: 0.89450\n",
            "[2,   119] loss: 0.93667\n",
            "[2,   120] loss: 0.95163\n",
            "[2,   121] loss: 0.96605\n",
            "[2,   122] loss: 0.96109\n",
            "[2,   123] loss: 0.86481\n",
            "[2,   124] loss: 0.91518\n",
            "[2,   125] loss: 0.92311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet(in_ch=6, out_ch=1, use_dropout=False).to(dev)\n",
        "\n",
        "for epoch in range(opt['epochs']):  # loop over the dataset multiple times\n",
        "    \n",
        "    train_metrics = initialize_metrics()\n",
        "    val_metrics = initialize_metrics()\n",
        "\n",
        "    \"\"\"\n",
        "    Begin Training\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    logging.info('SET model mode to train!')\n",
        "    batch_iter = 0\n",
        "    #tbar = tqdm(train_loader)\n",
        "\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [batch_img1, batch_img2, labels]\n",
        "        batch_img1, batch_img2, labels = data\n",
        "        \n",
        "        # Set variables for training\n",
        "        batch_img1 = batch_img1.float()\n",
        "        batch_img2 = batch_img2.float()\n",
        "        \n",
        "        inputs = torch.cat([batch_img1, batch_img2], dim=1).to(dev)\n",
        "\n",
        "        labels = torch.unsqueeze(labels, dim=1)\n",
        "        labels = labels.long().to(dev)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        # Get model predictions, calculate loss, backprop\n",
        "        cd_preds = model(inputs)\n",
        "\n",
        "        cd_loss = criterion(cd_preds, labels)\n",
        "        loss = cd_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        cd_preds = cd_preds[-1]\n",
        "        _, cd_preds = torch.max(cd_preds, 1)\n",
        "        \n",
        "        # Calculate and log other batch metrics\n",
        "        cd_corrects = (100 *\n",
        "                       (cd_preds.squeeze().byte() == labels.squeeze().byte()).sum() /\n",
        "                       (labels.size()[0] * (opt['batch_size']**2)))\n",
        "        \n",
        "        cd_train_report = prfs(labels.data.cpu().numpy().flatten(),\n",
        "                               cd_preds.data.cpu().numpy().flatten(),\n",
        "                               average='binary',\n",
        "                               zero_division=0,\n",
        "                               pos_label=1)\n",
        "        \n",
        "        train_metrics = set_metrics(train_metrics,\n",
        "                                    cd_loss,\n",
        "                                    cd_corrects,\n",
        "                                    cd_train_report,\n",
        "                                    scheduler.get_last_lr())\n",
        "        \n",
        "        # log the batch mean metrics\n",
        "        mean_train_metrics = get_mean_metrics(train_metrics)\n",
        "        \n",
        "        for k, v in mean_train_metrics.items():\n",
        "            writer.add_scalars(str(k), {'train': v}, total_step)\n",
        "\n",
        "        # clear batch variables from memory\n",
        "        del batch_img1, batch_img2, labels\n",
        "        \n",
        "        \n",
        "        # print statistics\n",
        "        if i % 1000 == 0:    # print every 2000 mini-batches\n",
        "            print(f\"[{opt['epoch'] + 1}, {i + 1:5d}] loss: {mean_train_metrics['cd_losses']:.5f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "    logging.info(\"EPOCH {} TRAIN METRICS\".format(epoch) + str(mean_train_metrics))\n",
        "    \n",
        "    break \n",
        "\n",
        "    \"\"\"\n",
        "    Begin Validation\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_img1, batch_img2, labels in val_loader:\n",
        "            # Set variables for training\n",
        "            batch_img1 = batch_img1.float()\n",
        "            batch_img2 = batch_img2.float()\n",
        "            \n",
        "            inputs = torch.cat([batch_img1, batch_img2], dim=1).to(dev)\n",
        "            labels = labels.long().to(dev)\n",
        "\n",
        "            # Get predictions and calculate loss\n",
        "            cd_preds = model(inputs)\n",
        "\n",
        "            cd_loss = criterion(cd_preds, labels)\n",
        "\n",
        "            cd_preds = cd_preds[-1]\n",
        "            _, cd_preds = torch.max(cd_preds, 1)\n",
        "\n",
        "            # Calculate and log other batch metrics\n",
        "            cd_corrects = (100 *\n",
        "                           (cd_preds.squeeze().byte() == labels.squeeze().byte()).sum() /\n",
        "                           (labels.size()[0] * (opt.patch_size**2)))\n",
        "\n",
        "            cd_val_report = prfs(labels.data.cpu().numpy().flatten(),\n",
        "                                 cd_preds.data.cpu().numpy().flatten(),\n",
        "                                 average='binary',\n",
        "                                 zero_division=0,\n",
        "                                 pos_label=1)\n",
        "\n",
        "            val_metrics = set_metrics(val_metrics,\n",
        "                                      cd_loss,\n",
        "                                      cd_corrects,\n",
        "                                      cd_val_report,\n",
        "                                      scheduler.get_last_lr())\n",
        "\n",
        "            # log the batch mean metrics\n",
        "            mean_val_metrics = get_mean_metrics(val_metrics)\n",
        "\n",
        "            for k, v in mean_train_metrics.items():\n",
        "                writer.add_scalars(str(k), {'val': v}, total_step)\n",
        "\n",
        "            # clear batch variables from memory\n",
        "            del batch_img1, batch_img2, labels    \n",
        "\n",
        "        \n",
        "        \"\"\"\n",
        "        Store the weights of good epochs based on validation results\n",
        "        \"\"\"\n",
        "        if ((mean_val_metrics['cd_precisions'] > best_metrics['cd_precisions'])\n",
        "                or\n",
        "                (mean_val_metrics['cd_recalls'] > best_metrics['cd_recalls'])\n",
        "                or\n",
        "                (mean_val_metrics['cd_f1scores'] > best_metrics['cd_f1scores'])):\n",
        "\n",
        "            # Insert training and epoch information to metadata dictionary\n",
        "            logging.info('updata the model')\n",
        "            opt['validation_metrics'] = mean_val_metrics\n",
        "\n",
        "            # Save model and log\n",
        "            if not os.path.exists('./tmp'):\n",
        "                os.mkdir('./tmp')\n",
        "            with open('./tmp/metadata_epoch_' + str(epoch) + '.json', 'w') as fout:\n",
        "                json.dump(opt, fout)\n",
        "\n",
        "            torch.save(model, './tmp/checkpoint_epoch_'+str(epoch)+'.pt')\n",
        "\n",
        "            # comet.log_asset(upload_metadata_file_path)\n",
        "            best_metrics = mean_val_metrics   \n",
        "            \n",
        "            \n",
        "        print('An epoch finished.')\n",
        "        \n",
        "writer.close()  # close tensor board\n",
        "logging.info('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "uo8DjbzEX3zX",
        "outputId": "94117855-b309-46f4-f936-9030e524dbe8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-e802eb08436d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Get model predictions, calculate loss, backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mcd_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcd_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcd_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/CDD_V1_2022-12-08/models/FC_EF_2018.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Stage 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mx11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0mx12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mx1p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/CDD_V1_2022-12-08/models/FC_EF_2018.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 3, 3, 3], expected input[16, 6, 258, 258] to have 3 channels, but got 6 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "xkUjSbTYZ1uD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}